{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained YOLO Model\n",
    "* Model varies from `YOLO11n`, `YOLO11s`, `YOLO11m`, `YOLO11l`, `YOLO11x`. (nano, small, medium, large, xlarge.)\n",
    "    - These are pretrained on COCO dataset, only for detecting 80 pre-trained classes\n",
    "    - There are also models for segmentation, and pose detection.\n",
    "* In this code notebook, we will going to also try the ,,Track'' mode, which is available for all detect, segment, and pose models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "detect_model = YOLO(\"yolo11n.pt\")\n",
    "pose_model = YOLO(\"yolo11n-pose.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read your camera stream, track the objects as well as human pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 49.5ms\n",
      "Speed: 5.3ms preprocess, 49.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 64.8ms\n",
      "Speed: 0.7ms preprocess, 64.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 50.1ms\n",
      "Speed: 0.8ms preprocess, 50.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 49.9ms\n",
      "Speed: 0.7ms preprocess, 49.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 51.4ms\n",
      "Speed: 0.8ms preprocess, 51.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 46.7ms\n",
      "Speed: 0.6ms preprocess, 46.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 49.4ms\n",
      "Speed: 0.7ms preprocess, 49.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 44.8ms\n",
      "Speed: 0.6ms preprocess, 44.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 42.2ms\n",
      "Speed: 0.7ms preprocess, 42.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 44.8ms\n",
      "Speed: 0.8ms preprocess, 44.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 48.5ms\n",
      "Speed: 0.7ms preprocess, 48.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 51.6ms\n",
      "Speed: 0.9ms preprocess, 51.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 50.8ms\n",
      "Speed: 0.7ms preprocess, 50.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 42.1ms\n",
      "Speed: 0.7ms preprocess, 42.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 40.0ms\n",
      "Speed: 0.7ms preprocess, 40.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 43.7ms\n",
      "Speed: 0.8ms preprocess, 43.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 41.6ms\n",
      "Speed: 0.7ms preprocess, 41.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 41.3ms\n",
      "Speed: 0.6ms preprocess, 41.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 microwave, 38.0ms\n",
      "Speed: 0.8ms preprocess, 38.0ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 41.4ms\n",
      "Speed: 0.6ms preprocess, 41.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 microwave, 36.9ms\n",
      "Speed: 0.7ms preprocess, 36.9ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.1ms\n",
      "Speed: 0.6ms preprocess, 39.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 microwave, 35.9ms\n",
      "Speed: 0.7ms preprocess, 35.9ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 40.3ms\n",
      "Speed: 0.7ms preprocess, 40.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 microwave, 36.6ms\n",
      "Speed: 0.8ms preprocess, 36.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.7ms\n",
      "Speed: 0.7ms preprocess, 39.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.0ms\n",
      "Speed: 0.8ms preprocess, 37.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.8ms\n",
      "Speed: 0.6ms preprocess, 39.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.8ms\n",
      "Speed: 0.7ms preprocess, 39.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 41.8ms\n",
      "Speed: 0.6ms preprocess, 41.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 36.2ms\n",
      "Speed: 0.8ms preprocess, 36.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.8ms\n",
      "Speed: 0.6ms preprocess, 39.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.7ms\n",
      "Speed: 0.7ms preprocess, 37.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 36.9ms\n",
      "Speed: 0.6ms preprocess, 36.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 35.8ms\n",
      "Speed: 0.7ms preprocess, 35.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.0ms\n",
      "Speed: 0.6ms preprocess, 39.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 35.1ms\n",
      "Speed: 0.9ms preprocess, 35.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.6ms\n",
      "Speed: 0.6ms preprocess, 38.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 35.2ms\n",
      "Speed: 0.7ms preprocess, 35.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 43.2ms\n",
      "Speed: 0.8ms preprocess, 43.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 35.8ms\n",
      "Speed: 0.7ms preprocess, 35.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.8ms\n",
      "Speed: 0.8ms preprocess, 38.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 35.3ms\n",
      "Speed: 0.8ms preprocess, 35.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.9ms\n",
      "Speed: 0.6ms preprocess, 38.9ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 35.8ms\n",
      "Speed: 0.7ms preprocess, 35.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.9ms\n",
      "Speed: 0.7ms preprocess, 38.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2 wine glasss, 38.5ms\n",
      "Speed: 0.7ms preprocess, 38.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.9ms\n",
      "Speed: 0.6ms preprocess, 38.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.1ms\n",
      "Speed: 0.7ms preprocess, 39.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 49.5ms\n",
      "Speed: 0.7ms preprocess, 49.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 46.4ms\n",
      "Speed: 0.7ms preprocess, 46.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 42.0ms\n",
      "Speed: 0.7ms preprocess, 42.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 microwave, 37.8ms\n",
      "Speed: 0.8ms preprocess, 37.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 40.2ms\n",
      "Speed: 0.6ms preprocess, 40.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 microwave, 35.7ms\n",
      "Speed: 0.7ms preprocess, 35.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.4ms\n",
      "Speed: 0.6ms preprocess, 37.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 microwave, 34.3ms\n",
      "Speed: 0.7ms preprocess, 34.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 53.1ms\n",
      "Speed: 0.8ms preprocess, 53.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.9ms\n",
      "Speed: 0.7ms preprocess, 37.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 40.7ms\n",
      "Speed: 0.6ms preprocess, 40.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.2ms\n",
      "Speed: 0.7ms preprocess, 37.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 45.1ms\n",
      "Speed: 0.7ms preprocess, 45.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.3ms\n",
      "Speed: 0.7ms preprocess, 37.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 53.6ms\n",
      "Speed: 0.7ms preprocess, 53.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 microwave, 46.9ms\n",
      "Speed: 0.7ms preprocess, 46.9ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 43.7ms\n",
      "Speed: 0.7ms preprocess, 43.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.0ms\n",
      "Speed: 0.8ms preprocess, 39.0ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 41.9ms\n",
      "Speed: 0.7ms preprocess, 41.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 microwave, 40.2ms\n",
      "Speed: 0.8ms preprocess, 40.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 41.9ms\n",
      "Speed: 0.7ms preprocess, 41.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 41.5ms\n",
      "Speed: 0.8ms preprocess, 41.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 42.2ms\n",
      "Speed: 0.7ms preprocess, 42.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 36.7ms\n",
      "Speed: 0.7ms preprocess, 36.7ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 41.0ms\n",
      "Speed: 0.8ms preprocess, 41.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.1ms\n",
      "Speed: 0.8ms preprocess, 38.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 41.3ms\n",
      "Speed: 0.8ms preprocess, 41.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 36.4ms\n",
      "Speed: 1.0ms preprocess, 36.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 40.1ms\n",
      "Speed: 0.6ms preprocess, 40.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 microwave, 42.7ms\n",
      "Speed: 0.7ms preprocess, 42.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 41.4ms\n",
      "Speed: 0.6ms preprocess, 41.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 microwave, 39.1ms\n",
      "Speed: 0.8ms preprocess, 39.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.6ms\n",
      "Speed: 0.9ms preprocess, 39.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 microwave, 43.6ms\n",
      "Speed: 0.7ms preprocess, 43.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 46.3ms\n",
      "Speed: 0.9ms preprocess, 46.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 43.8ms\n",
      "Speed: 0.7ms preprocess, 43.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 47.5ms\n",
      "Speed: 0.6ms preprocess, 47.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 microwave, 49.0ms\n",
      "Speed: 0.8ms preprocess, 49.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 45.7ms\n",
      "Speed: 0.7ms preprocess, 45.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 51.0ms\n",
      "Speed: 0.9ms preprocess, 51.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 45.2ms\n",
      "Speed: 0.8ms preprocess, 45.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 41.4ms\n",
      "Speed: 0.7ms preprocess, 41.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 53.1ms\n",
      "Speed: 0.7ms preprocess, 53.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 microwave, 50.4ms\n",
      "Speed: 0.7ms preprocess, 50.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 44.4ms\n",
      "Speed: 1.0ms preprocess, 44.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 microwave, 38.7ms\n",
      "Speed: 0.7ms preprocess, 38.7ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.5ms\n",
      "Speed: 0.7ms preprocess, 39.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 36.4ms\n",
      "Speed: 0.7ms preprocess, 36.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.5ms\n",
      "Speed: 0.6ms preprocess, 39.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 49.8ms\n",
      "Speed: 0.7ms preprocess, 49.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 67.4ms\n",
      "Speed: 2.4ms preprocess, 67.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 44.4ms\n",
      "Speed: 0.7ms preprocess, 44.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.4ms\n",
      "Speed: 0.7ms preprocess, 39.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 38.6ms\n",
      "Speed: 0.7ms preprocess, 38.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 42.2ms\n",
      "Speed: 0.7ms preprocess, 42.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 39.0ms\n",
      "Speed: 0.9ms preprocess, 39.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 55.1ms\n",
      "Speed: 0.7ms preprocess, 55.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 38.9ms\n",
      "Speed: 0.9ms preprocess, 38.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 42.0ms\n",
      "Speed: 0.9ms preprocess, 42.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 38.6ms\n",
      "Speed: 0.9ms preprocess, 38.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 54.5ms\n",
      "Speed: 0.9ms preprocess, 54.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 48.3ms\n",
      "Speed: 0.8ms preprocess, 48.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.9ms\n",
      "Speed: 0.7ms preprocess, 38.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 35.7ms\n",
      "Speed: 0.8ms preprocess, 35.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 37.3ms\n",
      "Speed: 0.7ms preprocess, 37.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 38.3ms\n",
      "Speed: 0.7ms preprocess, 38.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 49.8ms\n",
      "Speed: 0.7ms preprocess, 49.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 255.5ms\n",
      "Speed: 3.4ms preprocess, 255.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 38.6ms\n",
      "Speed: 0.8ms preprocess, 38.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 35.6ms\n",
      "Speed: 0.8ms preprocess, 35.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.3ms\n",
      "Speed: 0.8ms preprocess, 39.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 49.0ms\n",
      "Speed: 0.9ms preprocess, 49.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 49.1ms\n",
      "Speed: 0.7ms preprocess, 49.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 39.6ms\n",
      "Speed: 0.8ms preprocess, 39.6ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 55.9ms\n",
      "Speed: 0.8ms preprocess, 55.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 54.5ms\n",
      "Speed: 0.9ms preprocess, 54.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 39.2ms\n",
      "Speed: 0.7ms preprocess, 39.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 40.0ms\n",
      "Speed: 0.8ms preprocess, 40.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 44.2ms\n",
      "Speed: 0.7ms preprocess, 44.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 44.3ms\n",
      "Speed: 0.9ms preprocess, 44.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 42.4ms\n",
      "Speed: 0.8ms preprocess, 42.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 banana, 1 microwave, 39.0ms\n",
      "Speed: 0.8ms preprocess, 39.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 41.2ms\n",
      "Speed: 0.8ms preprocess, 41.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 remote, 1 microwave, 38.9ms\n",
      "Speed: 0.8ms preprocess, 38.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 46.8ms\n",
      "Speed: 0.8ms preprocess, 46.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import copy\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # if frame is read correctly ret is True\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "    \n",
    "    # Use below code if your face looks blue.\n",
    "    # rgb = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    det_res = detect_model.track(source=frame, show=False)[0]\n",
    "    pos_res = pose_model.track(source=frame, show=False)[0]\n",
    "\n",
    "    all_res = copy.deepcopy(det_res)\n",
    "    all_res.keypoints = pos_res.keypoints\n",
    "\n",
    "    res_img = all_res.plot()\n",
    "\n",
    "    cv.imshow('result', res_img)\n",
    "    k = cv.waitKey(1)\n",
    "\n",
    "    if k == ord('q'):\n",
    "        break\n",
    "    \n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run below cell if the opencv widget forced to be closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyaudio in /home/hyemin/anaconda3/envs/py310/lib/python3.10/site-packages (0.2.14)\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U openai-whisper\n",
    "# !pip install numpy==2.0\n",
    "# !pip3 install pvrecorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 72.1M/72.1M [00:05<00:00, 13.6MiB/s]\n",
      "/home/hyemin/anaconda3/envs/py310/lib/python3.10/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import wave\n",
    "from pvrecorder import PvRecorder\n",
    "import time\n",
    "\n",
    "def listen(output_path='tmp.wav'):\n",
    "    device_index = -1\n",
    "\n",
    "    recorder = PvRecorder(frame_length=1024, device_index=device_index)\n",
    "    recorder.start()\n",
    "\n",
    "    wavfile = None\n",
    "\n",
    "    if output_path is not None:\n",
    "        wavfile = wave.open(output_path, \"w\")\n",
    "        # noinspection PyTypeChecker\n",
    "        wavfile.setparams((1, 2, recorder.sample_rate, recorder.frame_length, \"NONE\", \"NONE\"))\n",
    "\n",
    "    st = time.time()\n",
    "    print(\"=======Start Listening\")\n",
    "            \n",
    "    while True:\n",
    "        frame = recorder.read()\n",
    "        if wavfile is not None:\n",
    "            wavfile.writeframes(struct.pack(\"h\" * len(frame), *frame))\n",
    "        if time.time()-st > 10:\n",
    "            print(\"=======Stopping Listening\")\n",
    "            break\n",
    "\n",
    "    recorder.delete()\n",
    "    if wavfile is not None:\n",
    "        wavfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def understand(filename='tmp.wav'):\n",
    "    audio = whisper.load_audio(filename)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "    mel = whisper.log_mel_spectrogram(audio)\n",
    "    result = whisper.decode(model, mel, whisper.DecodingOptions())\n",
    "\n",
    "    return result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "ollama.pull('gemma2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======Start Listening\n",
      "=======Stopping Listening\n",
      "=======Input: Hello, how are you?\n",
      "=======Output: I'm doing well, thanks! How about you? 😊  \n",
      "=======Start Listening\n",
      "=======Stopping Listening\n",
      "=======Input: For me it's fine and I need to do some works.\n",
      "=======Output: Sounds good! Get those works done.  👍 \n",
      "\n",
      "Let me know if you need anything while you're at it!\n",
      "=======Start Listening\n",
      "=======Stopping Listening\n",
      "=======Input: But I need to talk with someone because I don't want to do the work so much so I need some things\n",
      "=======Output: Sounds tough. What kind of things are you needing help with? Maybe I can give you some ideas.  🤔 \n",
      "\n",
      "=======Start Listening\n",
      "=======Stopping Listening\n",
      "=======Input: Maybe you can try some class materials that I need to make.\n",
      "=======Output: What kind of class materials? Tell me more!  📚✏️  \n",
      "\n",
      "=======Start Listening\n",
      "=======Stopping Listening\n",
      "=======Input: I need to make a slide for human robot interaction.\n",
      "=======Output: Got it!  What kind of information do you want on the slide? 🧑‍💻🤔  \n",
      "\n",
      "For example: examples, definitions, challenges...? ✨\n",
      "=======Start Listening\n",
      "=======Stopping Listening\n",
      "=======Input: definitions, challenges, particles. I would like to search some examples on that researches on that HRI.\n",
      "=======Output: You want to know about definitions, challenges, and particles in Human-Robot Interaction (HRI) research?\n",
      "\n",
      "**Me:**  Think of HRI as how humans and robots work together. Definitions are all about understanding what makes a good interaction, what we expect from each other.\n",
      "\n",
      "**Challenges:** Making robots understand us better, like our emotions and intentions. Also making interactions natural and safe for humans. \n",
      "\n",
      "**Particles?** Hmm... maybe you mean sensor data like touch or force feedback? Those help robots \"feel\" the world and interact more naturally.\n",
      "\n",
      "**Examples:** Search \"social robotics,\" \"robot learning from human demonstration,\" or \"haptic feedback in HRI.\"\n",
      "\n",
      "\n",
      "Let me know if you have more questions! \n",
      "\n",
      "=======Start Listening\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbye\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39mlower():\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     text \u001b[38;5;241m=\u001b[39m understand()\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=======Input:\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n",
      "Cell \u001b[0;32mIn[29], line 23\u001b[0m, in \u001b[0;36mlisten\u001b[0;34m(output_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=======Start Listening\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[43mrecorder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wavfile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m         wavfile\u001b[38;5;241m.\u001b[39mwriteframes(struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(frame), \u001b[38;5;241m*\u001b[39mframe))\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/pvrecorder/_pvrecorder.py:176\u001b[0m, in \u001b[0;36mPvRecorder.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Synchronous call to read a frame of audio.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m:return: A frame with size `frame_length` matching the value given to `__init__()`.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    175\u001b[0m pcm \u001b[38;5;241m=\u001b[39m (c_int16 \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frame_length)()\n\u001b[0;32m--> 176\u001b[0m status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpcm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPvRecorderStatuses\u001b[38;5;241m.\u001b[39mSUCCESS:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_PVRECORDER_STATUS_TO_EXCEPTION[status](\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to read from device.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/enum.py:359\u001b[0m, in \u001b[0;36mEnumMeta.__call__\u001b[0;34m(cls, value, names, module, qualname, type, start)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    classes/types should always be True.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, value, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, qualname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Either returns an existing member, or creates a new enum class.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m    `type`, if set, will be mixed in as the first base class.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# simple value lookup\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from lerobot.common.utils.utils import log_say\n",
    "\n",
    "text = 'start'\n",
    "while 'bye' not in text.lower():\n",
    "    listen()\n",
    "    text = understand()\n",
    "    print('=======Input:', text)\n",
    "    response = ollama.chat(model='gemma2', messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': 'Please always answer to me in 50 words. INPUT: [' + text + ']',\n",
    "      },\n",
    "    ])\n",
    "    print('=======Output:', response['message']['content'],)\n",
    "    log_say(response['message']['content'], True)\n",
    "    time.sleep(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
